{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWvHMA1YctLrnA3o25C0oh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakilbaki/ml-2025/blob/main/02_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bp9aS-1jrQil"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ],
      "metadata": {
        "id": "RrWpZPG9r27u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "metadata": {
        "id": "yDBKUpquuitm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "IMAGES_PATH = Path() / \"images\" / \"classification\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "H-prLeEnupIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST = Modified National Institute of Standards and Technology database.\n",
        "\n",
        "C’est un jeu de données hyper classique de chiffres manuscrits.\n",
        "\n",
        "Contenu :\n",
        "\n",
        "70 000 images\n",
        "\n",
        "Chaque image = 28x28 pixels ➔ soit 784 features (28x28 = 784).\n",
        "\n",
        "Les valeurs sont en niveaux de gris (0-255).\n",
        "\n",
        "Chaque image est une représentation d’un chiffre entre 0 et 9.\n",
        "\n"
      ],
      "metadata": {
        "id": "_CPYkz0ou8f5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ce dataset a été crée par yann le cun qui est un recheur francais qui travail en ia qui le chief ia scientiest chez meta et c'est lui qui a popularisé les cnn  et c'est lui crée lenet\n",
        "Le premier vrai \"succès industriel\" de LeNet = lire automatiquement les montants sur les chèques de banque aux USA."
      ],
      "metadata": {
        "id": "mUklStOivkNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', as_frame=False)"
      ],
      "metadata": {
        "id": "GITzOpLZwP--"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.keys()  # extra code – we only use data and target in this notebook\n"
      ],
      "metadata": {
        "id": "g6uDDislwaJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = mnist.data, mnist.target"
      ],
      "metadata": {
        "id": "QKobZzKDwrWr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour pouvoir utiliser scikit-learn et les modèles ML classiques, tu dois aplatir tes images en vecteurs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tLbroI3OxvG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_digit(image_data):\n",
        "    image = image_data.reshape(28, 28)\n",
        "    plt.imshow(image, cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "some_digit = X[0]\n",
        "plot_digit(some_digit)\n",
        "save_fig(\"some_digit_plot\")  # extra code\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xwaQzqbnxxl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ce code permet de reconstruire une image à partir de sa forme matricielle\n"
      ],
      "metadata": {
        "id": "ezR7DiYoyBv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – this cell generates and saves Figure 3–2\n",
        "plt.figure(figsize=(9, 9))\n",
        "for idx, image_data in enumerate(X[:100]):\n",
        "    plt.subplot(10, 10, idx + 1)\n",
        "    plot_digit(image_data)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "save_fig(\"more_digits_plot\", tight_layout=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "civqbGPlyH7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "le slicing est tres important ici , il a permis de recuperer les 100 premières lignes de X\n"
      ],
      "metadata": {
        "id": "ljUyfQAyy4f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n"
      ],
      "metadata": {
        "id": "NpmVSIXrzEvh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "on peut facilement faire ca avec train_test_split\n"
      ],
      "metadata": {
        "id": "RaHRsNaGzQUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training a binary classifier\n"
      ],
      "metadata": {
        "id": "7HhfURxHz27V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m5RgtEjx5Nu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_5 = (y_train == '5')  # True for all 5s, False for all other digits\n",
        "y_test_5 = (y_test == '5')"
      ],
      "metadata": {
        "id": "rS_cXHnuz99_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "si y_train==5 => True else false\n",
        "c'est ceux dont on a besoin pour notre classification binaire\n"
      ],
      "metadata": {
        "id": "n-LYpLvw0F4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(random_state=42)\n",
        "sgd_clf.fit(X_train, y_train_5)"
      ],
      "metadata": {
        "id": "EPpj52cl0iyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "on fixe la seed encore à 42 = valeur par défaut en datascience\n",
        "## pour une classification binaire il faut que la target soit binaire aussi\n",
        "# ici on utilise le stochastic gradient descent classifier"
      ],
      "metadata": {
        "id": "DiP_q2am0mNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on apprentissage supérvisé on cherche à mimiser une fonction cout\n",
        "on a alors la deux option :\n",
        "##LogisticRegression() = modèle linéaire + solveur optimisé\n",
        "➔ très précis, bon pour des datasets moyens ou petits.\n",
        "\n",
        "##SGDClassifier() = modèle linéaire + entraînement par descente de gradient stochastique\n",
        "➔ rapide et scalable sur gros datasets.\n",
        "du coup l'approche stochastique est probabiliste\n",
        "\n"
      ],
      "metadata": {
        "id": "xNKvQGmb1oAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_clf.predict([some_digit])\n"
      ],
      "metadata": {
        "id": "nYOirsLD4aA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "le modéle prédit bien\n",
        "On sépare toujours nos données :\n",
        "Train set : pour entraîner le modèle\n",
        "\n",
        "Test set : pour évaluer la capacité à généraliser\n",
        "\n",
        "(parfois aussi un Validation set pour régler les hyperparamètres)"
      ],
      "metadata": {
        "id": "-80G2bxZ5gAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# *testons maintenant les performances du modèles *\n",
        "\n"
      ],
      "metadata": {
        "id": "2JNXFlsA6IuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
      ],
      "metadata": {
        "id": "qA_WT-2O6Hdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "le but de ce code est de voir la capacité d'apprentissage du modéle en utilisant la validations croisé"
      ],
      "metadata": {
        "id": "vpjYRjEuoKKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import clone\n",
        "\n",
        "skfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is not\n",
        "                                       # already shuffled\n",
        "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
        "    clone_clf = clone(sgd_clf)\n",
        "    X_train_folds = X_train[train_index]\n",
        "    y_train_folds = y_train_5[train_index]\n",
        "    X_test_fold = X_train[test_index]\n",
        "    y_test_fold = y_train_5[test_index]\n",
        "\n",
        "    clone_clf.fit(X_train_folds, y_train_folds)\n",
        "    y_pred = clone_clf.predict(X_test_fold)\n",
        "    n_correct = sum(y_pred == y_test_fold)\n",
        "    print(n_correct / len(y_pred))"
      ],
      "metadata": {
        "id": "b271kUvHoWqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ce code montre comment on peut faire la validarion croisee  \n",
        "StratifiedKFold(n_splits=3)\n",
        "aprés boucle sur une méthode de l'objet skfold"
      ],
      "metadata": {
        "id": "n1v_HHhZo_hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy_clf = DummyClassifier()\n",
        "dummy_clf.fit(X_train, y_train_5)\n",
        "print(any(dummy_clf.predict(X_train)))"
      ],
      "metadata": {
        "id": "Osdtbqi5qP1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ici on utilise le dummy classifier pour tester notre modéle , celui ci predit toujours le resultat le plus fréquent**\n",
        "# l'intéret est de voir si le modéle a vraiment appris qlq chose ou si il est inutile"
      ],
      "metadata": {
        "id": "lmWMdfKBqQ1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n"
      ],
      "metadata": {
        "id": "xxonFko0q9SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# accuracy_dummy< accuracy_sgd du coup le modèle a bien appris qlq chose\n",
        "\n",
        "\n",
        "#  ⚠ le dummy classifier peut etre utiliser sur la classification multidimontionnel"
      ],
      "metadata": {
        "id": "AbkBOa5qrABn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "condusion matrix"
      ],
      "metadata": {
        "id": "860zIEXBroEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
      ],
      "metadata": {
        "id": "bqLzJzvDrtE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Très intéressent prediction basé sur le meme principe de la  cross_validation j'aime l'idée"
      ],
      "metadata": {
        "id": "M6ufbfTBx4x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_train_5, y_train_pred)\n",
        "cm"
      ],
      "metadata": {
        "id": "d8PqO7SMrv9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "la confusion marix contien les TN ET LES TF sur les diagonales et les FN FP"
      ],
      "metadata": {
        "id": "TcBVa1gGsHml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L’accuracy seule peut cacher des problèmes (surtout en dataset déséquilibré).\n",
        "\n",
        "Avec la confusion matrix ➔ tu vois si :\n",
        "\n",
        "Ton modèle détecte bien les 5 (TP / FN).\n",
        "\n",
        "Ton modèle fait beaucoup de fausses alarmes (FP)."
      ],
      "metadata": {
        "id": "_GX7nqrDs-eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision_score(y_train_5, y_train_pred)  # == 3530 / (687 + 3530)= TP/TP+FP \tParmi les \"5\" que j’ai prédit, combien étaient vraiment des \"5\""
      ],
      "metadata": {
        "id": "jmRl2Pp3tFuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBALISTIQUEMENT : ca donne le pourcentage de reuissite si un 5 est predit\n",
        "\n"
      ],
      "metadata": {
        "id": "VNUV0_cqt52i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y_train_5, y_train_pred)  # == 3530 / (1891 + 3530) (TP/TP+FN) ( Parmi tous les vrais \"5\", combien j’en ai correctement détecté  )\n"
      ],
      "metadata": {
        "id": "gm-SDsBCvDCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problème\tOn privilégie quoi ?\n",
        "Détection de cancer\tRecall\n",
        "Filtrage anti-spam\tPrecision\n",
        "Reconnaissance d’empreintes\tPrecision\n",
        "Reconnaissance de visages en sécurité\tRecall\n",
        "\n",
        "afin de mieux comprendre pourquoi rappele toi que pour le cancer on utilise le recall et pour le filtrage anti-spam on utlilise la précision  \n"
      ],
      "metadata": {
        "id": "Af64j6K55MDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_train_5, y_train_pred)"
      ],
      "metadata": {
        "id": "kkkERsKlE2Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ici on voit le f1_score= 2 (r*p/r+p) il permet de mesurer les 2 cad à qu'elle point le modele est bon dans la precision et le recall\n"
      ],
      "metadata": {
        "id": "uGHEQmc9FJZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suis-je globalement bon dans les deux ?"
      ],
      "metadata": {
        "id": "6d3-wuQwFrIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#recall et tradeoff\n",
        "y_scores = sgd_clf.decision_function([some_digit])\n",
        "y_scores"
      ],
      "metadata": {
        "id": "jDyCE0eaF976"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ça renvoie le score de distance par rapport à l'hyperplan de décision.\n",
        "Plus le score est grand ➔ plus le modèle est confiant que c’est un \"5\".\n",
        "Plus le score est négatif ➔ plus il est sûr que ce n’est pas un \"5\""
      ],
      "metadata": {
        "id": "AscpTDKyGqDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le cœur de presque tous les modèles linéaires (et même beaucoup de non-linéaires) repose sur cette fameuse expression :𝑤⋅x+𝑏\n",
        "ce score est la distance entre la prediction et l'hyperplan de décision"
      ],
      "metadata": {
        "id": "7ztxsL71H4dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0\n",
        "\n",
        "y_some_digit_pred = (y_scores > threshold)\n"
      ],
      "metadata": {
        "id": "ry1B_W3nIYW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_some_digit_pred\n",
        "#il faut remarquer que pour un threshold égale à 0 le resultat est le méme qu'appeler la fonction predict\n"
      ],
      "metadata": {
        "id": "uj2XsXw3IgMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 3000\n",
        "y_some_digit_pred = (y_scores > threshold)\n",
        "y_some_digit_pred"
      ],
      "metadata": {
        "id": "UIB3rneUJ1_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
        "                             method=\"decision_function\")\n",
        "#ici le code va retourner les sortie de la decision function d(x)=wx+b\n"
      ],
      "metadata": {
        "id": "NPWrVC21KQUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "l'intéret de ca et pour après càd que ca permettra de choisir le threashold optimal pour toute precision voulu"
      ],
      "metadata": {
        "id": "TrQH8GnXOpjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
      ],
      "metadata": {
        "id": "QcLmuBFqKloC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))  # extra code – it's not needed, just formatting\n",
        "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
        "\n",
        "# extra code – this section just beautifies and saves Figure 3–5\n",
        "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
        "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
        "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
        "plt.axis([-50000, 50000, 0, 1])\n",
        "plt.grid()\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.legend(loc=\"center right\")\n",
        "save_fig(\"precision_recall_vs_threshold_plot\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NEWuvSBIMHXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfaOynDLMWOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le seuil optimal ne sort pas magiquement du modèle.\n",
        "Tu dois l’ajuster en fonction de ce que tu veux (médecine, finance, sécurité...)."
      ],
      "metadata": {
        "id": "YP45S_xiMYJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as patches  # extra code – for the curved arrow\n",
        "\n",
        "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
        "\n",
        "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
        "\n",
        "# extra code – just beautifies and saves Figure 3–6\n",
        "plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
        "plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
        "plt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
        "         label=\"Point at threshold 3,000\")\n",
        "plt.gca().add_patch(patches.FancyArrowPatch(\n",
        "    (0.79, 0.60), (0.61, 0.78),\n",
        "    connectionstyle=\"arc3,rad=.2\",\n",
        "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
        "    color=\"#444444\"))\n",
        "plt.text(0.56, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.grid()\n",
        "plt.legend(loc=\"lower left\")\n",
        "save_fig(\"precision_vs_recall_plot\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z2c-S1wFMvhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ce code met en évidence le threshold optimal pour avoir 90% de precision correspond à 3000"
      ],
      "metadata": {
        "id": "oDrHO3RjNT0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
        "## ici on cherche le premier seuil\n",
        "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
        "threshold_for_90_precision"
      ],
      "metadata": {
        "id": "zWTGtrvmNTTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_90 = (y_scores >= threshold_for_90_precision) #true id y_scores > Th else falsed\n",
        "precision_score(y_train_5, y_train_pred_90)\n"
      ],
      "metadata": {
        "id": "1IHNo4J2OCRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# on filtre les seuils qui respectent la contrainte\n",
        "valid = np.where(recalls >= 0.90)[0]\n",
        "# on prend le plus grand seuil parmi ceux qui respectent la contrainte du coup le argmax ne marche pas\n",
        "best_idx = valid[-1]  # dernier élément compatible\n",
        "threshold_for_90_recall = thresholds[best_idx]\n",
        "threshold_for_90_recall\n",
        "y_train_pred_90r = (y_scores >= threshold_for_90_recall) #true id y_scores > Th else falsed\n",
        "recall_score(y_train_5, y_train_pred_90r)\n",
        "\n"
      ],
      "metadata": {
        "id": "hApGjymlQLdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE ROC CURVE"
      ],
      "metadata": {
        "id": "aLy7F24aTcF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
        "\n",
        "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
        "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
        "\n",
        "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
        "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
        "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
        "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
        "\n",
        "# extra code – just beautifies and saves Figure 3–7\n",
        "plt.gca().add_patch(patches.FancyArrowPatch(\n",
        "    (0.20, 0.89), (0.07, 0.70),\n",
        "    connectionstyle=\"arc3,rad=.4\",\n",
        "    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n",
        "    color=\"#444444\"))\n",
        "plt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\n",
        "plt.xlabel('False Positive Rate (Fall-Out)')\n",
        "plt.ylabel('True Positive Rate (Recall)')\n",
        "plt.grid()\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.legend(loc=\"lower right\", fontsize=13)\n",
        "save_fig(\"roc_curve_plot\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Ag1Gn5Jhx-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "precision_recall Très bon pour datasets déséquilibrés (comme ton MNIST 5 ou pas 5)\n",
        "ROC\tBon pour évaluer globalement la capacité de discrimination du modèle\n"
      ],
      "metadata": {
        "id": "SJotAJpQihuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "roc_auc_score(y_train_5, y_scores)"
      ],
      "metadata": {
        "id": "E12vm7ztkbC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Étape\tCe que tu fais\n",
        "1. Entraînement\tTu entraînes 5 modèles\n",
        "2. Évaluation AUC\tTu calcules roc_auc_score() sur val set\n",
        "3. Sélection modèle\tTu gardes celui avec meilleur AUC\n",
        "4. Calibration de seuil\tTu fais precision_recall_curve() et choisis ton seuil métier\n",
        "5. Déploiement\tTu pars en production avec ton modèle calibré"
      ],
      "metadata": {
        "id": "bx0YevLfk9vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC ROC = vision globale \"mon modèle voit-il quelque chose ?\"\n",
        "Precision-Recall = vision fine \"comment il se comporte dans mon métier concret ?"
      ],
      "metadata": {
        "id": "iJCvcnPKm1U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = RandomForestClassifier(random_state=42)"
      ],
      "metadata": {
        "id": "cwIx0_4PnaJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
        "                                    method=\"predict_proba\")"
      ],
      "metadata": {
        "id": "aNihE8Kcnfj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_probas_forest[:3]\n"
      ],
      "metadata": {
        "id": "oaxudscMoJqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_50_to_60 = (y_probas_forest[:, 1] > 0.50) & (y_probas_forest[:, 1] < 0.60)\n",
        "print(f\"{(y_train_5[idx_50_to_60]).sum() / idx_50_to_60.sum():.1%}\")"
      ],
      "metadata": {
        "id": "vKCT6FnBokTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "➔ La calibration n’est utile que quand tu as besoin que les probabilités soient fiables.\n",
        "Un modèle calibré = ses probabilités sont \"honnêtes\" et reflètent la vraie fréquence d’occurrence.\n",
        "\n"
      ],
      "metadata": {
        "id": "THRRenHUq7-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline d'évaluation"
      ],
      "metadata": {
        "id": "2BI2DYWhtDfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores_forest = y_probas_forest[:, 1]\n",
        "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\n",
        "    y_train_5, y_scores_forest)\n"
      ],
      "metadata": {
        "id": "LNMeNk3ps-AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
        "\n",
        "plt.plot(recalls_forest, precisions_forest, \"b-\", linewidth=2,\n",
        "         label=\"Random Forest\")\n",
        "plt.plot(recalls, precisions, \"--\", linewidth=2, label=\"SGD\")\n",
        "\n",
        "# extra code – just beautifies and saves Figure 3–8\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.grid()\n",
        "plt.legend(loc=\"lower left\")\n",
        "save_fig(\"pr_curve_comparison_plot\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PYn3m02lsSIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_forest = y_probas_forest[:, 1] >= 0.5  # positive proba ≥ 50%\n",
        "f1_score(y_train_5, y_train_pred_forest)"
      ],
      "metadata": {
        "id": "S97lrTj1tlJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_train_5, y_scores_forest)\n"
      ],
      "metadata": {
        "id": "v9ifA8kBtwt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(y_train_5, y_train_pred_forest)\n"
      ],
      "metadata": {
        "id": "K6MqjQWBtzMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y_train_5, y_train_pred_forest)"
      ],
      "metadata": {
        "id": "VQay__tTt1Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part_2\n"
      ],
      "metadata": {
        "id": "QgMVWkyCe2PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification multi\n"
      ],
      "metadata": {
        "id": "aBU8UWaFI-q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the svm scales hardly on large dataset's that's why we will train it on 2000 samlples"
      ],
      "metadata": {
        "id": "d5H3vP2wofbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_clf = SVC(random_state=42)\n",
        "svm_clf.fit(X_train[:2000], y_train[:2000])"
      ],
      "metadata": {
        "id": "tXWGXJdGfVqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the svm draws the best line that gives the best marge\n",
        "marge = distance between the class and the hyperplan\n",
        "the closest points to the barriere define the support  vector\n",
        "the other points the svm dosen't see them"
      ],
      "metadata": {
        "id": "mSdhDFFjoxGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we can't trace a perfect line we introduce an error parametre c\n",
        "Grand C → moins d’erreurs autorisées → marge plus fine\n",
        "Petit C → plus d’erreurs autorisées → marge plus large → modèle plus généraliste\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "go9y1BN0qKWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. SVM non linéaire —\n",
        "le Kernel Trick\n",
        "Quand les données ne sont pas linéairement séparables (ex: spirale, cercles...), le SVM utilise des noyaux (kernels).\n",
        "L’idée : projeter les données dans un espace de dimension plus haute où elles deviennent linéairement séparables.\n",
        "\n",
        "linear → simple hyperplan\n",
        "\n",
        "rbf (radial basis function) → pour courbes complexes\n",
        "\n",
        "poly → polynômial\n",
        "\n"
      ],
      "metadata": {
        "id": "HgTA003Uqhnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Attention :\n",
        "Plus l’espace est complexe, plus le risque de surapprentissage est élevé (→ d’où l’intérêt de régulariser avec C et parfois gamma).\n",
        "Plus le kernel est complexe, plus l'entraînement est lent (surtout avec beaucoup de données)."
      ],
      "metadata": {
        "id": "GCdQYQwmrIr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_digit_scores = svm_clf.decision_function([some_digit])\n",
        "some_digit_scores.round(2)"
      ],
      "metadata": {
        "id": "TyOWyP4lqEBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_id = some_digit_scores.argmax()\n",
        "class_id"
      ],
      "metadata": {
        "id": "N-WjbGhPp9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the svm uses the one v one strategy it's like a league in multiclass"
      ],
      "metadata": {
        "id": "IvmmmNm0sy63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "ovr_clf = OneVsRestClassifier(SVC(random_state=42))\n",
        "ovr_clf.fit(X_train[:2000], y_train[:2000])"
      ],
      "metadata": {
        "id": "JkUZDoe6VjLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's a bit different than the ovo model how compare each two classes , this one compare one vs all the othere = 5 or not 5 so in this case you'll get 10 binary classifiers"
      ],
      "metadata": {
        "id": "EXwWi0L5WDr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in summary the svm just draws the lines then you schould the way you wanna do multiclass classification"
      ],
      "metadata": {
        "id": "nJOZ2RKcXWfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ovr_clf.predict([some_digit])"
      ],
      "metadata": {
        "id": "DRs9yLCDXiTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "svc uses ovo by default\n",
        "linearsvc uses ovr by default"
      ],
      "metadata": {
        "id": "Z-dFKjxyX1D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
      ],
      "metadata": {
        "id": "j1VEicWaYBeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(\"float64\"))\n",
        "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
      ],
      "metadata": {
        "id": "95BgXWERe989"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scalling is imported because the grandient descent based models the weights converge rapidly so it can be betwenn 0-1 and another feature between 0-1000"
      ],
      "metadata": {
        "id": "KLBdr6a9gHUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔑 Critères de choix\n",
        "Taille du jeu de données\n",
        "\n",
        "Petit (< 10 000 échantillons) → SVM, logistic, Naïve Bayes, k-NN\n",
        "\n",
        "Moyen (10 000–1 M) → Random Forest, Gradient Boosting, SVM linéaire (LinearSVC)\n",
        "\n",
        "Grand (> 1 M) → SGDClassifier, réseaux de neurones, LightGBM\n",
        "\n",
        "Complexité de la frontière\n",
        "\n",
        "Linéaire → Logistic, LinearSVC, SGDClassifier\n",
        "\n",
        "Non linéaire → SVC (kernel), Random Forest, Gradient Boosting\n",
        "\n",
        "Bruit & valeurs manquantes\n",
        "\n",
        "Robustes → arbres (RF, GB), Naïve Bayes\n",
        "\n",
        "Sensibles → SVM, k-NN, réseaux de neurones (nécessitent preprocessing)\n",
        "\n",
        "Exigences métier\n",
        "\n",
        "Interprétabilité → Logistic, arbres (avec export de règles), SHAP sur GB\n",
        "\n",
        "Temps réel / streaming → SGDClassifier, Perceptron\n",
        "\n",
        "Probas fiables → Logistic, calibrage (CalibratedClassifierCV sur RF/SVM)\n"
      ],
      "metadata": {
        "id": "ru4AvUjfi3XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSE D'ERREUR"
      ],
      "metadata": {
        "id": "ojjanyC2i-zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
        "plt.rc('font', size=9)  # extra code – make the text smaller\n",
        "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6j8pprAmjGiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "matrices très intéressente , elle montre comment le modéle se trompe et qu'elle classes il conffond"
      ],
      "metadata": {
        "id": "ghMyaMYIkHOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rc('font', size=10)  # extra code\n",
        "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
        "                                        normalize=\"true\", values_format=\".0%\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RU9yF7HZkcGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this cell the goal is to to see the recall (it's the diagonals) so  but in a different way than the binary you can say it gives the probability of confusing the true number with another number"
      ],
      "metadata": {
        "id": "Y7Ix1prYkd5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_weight = (y_train_pred != y_train)\n",
        "plt.rc('font', size=10)  # extra code\n",
        "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
        "                                        sample_weight=sample_weight,\n",
        "                                        normalize=\"true\", values_format=\".0%\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1XQYtZTyp1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this code choose if we had a false prediction wich are the probalies there another number to see with what he confused\n",
        "PS here the new parametre is the sample weight"
      ],
      "metadata": {
        "id": "DAvWcVLop0wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl_a, cl_b = '3', '5'\n",
        "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
        "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
        "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
        "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]"
      ],
      "metadata": {
        "id": "SctbLOnrrSNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – this cell generates and saves Figure 3–11\n",
        "size = 5\n",
        "pad = 0.2\n",
        "plt.figure(figsize=(size, size))\n",
        "for images, (label_col, label_row) in [(X_ba, (0, 0)), (X_bb, (1, 0)),\n",
        "                                       (X_aa, (0, 1)), (X_ab, (1, 1))]:\n",
        "    for idx, image_data in enumerate(images[:size*size]):\n",
        "        x = idx % size + label_col * (size + pad)\n",
        "        y = idx // size + label_row * (size + pad)\n",
        "        plt.imshow(image_data.reshape(28, 28), cmap=\"binary\",\n",
        "                   extent=(x, x + 1, y, y + 1))\n",
        "plt.xticks([size / 2, size + pad + size / 2], [str(cl_a), str(cl_b)])\n",
        "plt.yticks([size / 2, size + pad + size / 2], [str(cl_b), str(cl_a)])\n",
        "plt.plot([size + pad / 2, size + pad / 2], [0, 2 * size + pad], \"k:\")\n",
        "plt.plot([0, 2 * size + pad], [size + pad / 2, size + pad / 2], \"k:\")\n",
        "plt.axis([0, 2 * size + pad, 0, 2 * size + pad])\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "save_fig(\"error_analysis_digits_plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3M_SStLvra7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this cell help us see if the given numbers are really weird that's why they where misunterpreted then we can do feature engeniiring like adding a feature that calculate loop closure"
      ],
      "metadata": {
        "id": "ZJDHzggssShY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI_label classification"
      ],
      "metadata": {
        "id": "ECImPF7Ssuw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-label classification is the task where each sample can belong to zero, one, or multiple classes at the same time it's mostly used in tagging images , document categorization"
      ],
      "metadata": {
        "id": "SvDE0vJM58Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "y_train_large = (y_train >= '7')\n",
        "y_train_odd = (y_train.astype('int8') % 2 == 1)\n",
        "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_train, y_multilabel)"
      ],
      "metadata": {
        "id": "lr-TO5oa6HRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "knn classifier needs if k petit =>of k grand => uf // tres sensible à la scale des featurs"
      ],
      "metadata": {
        "id": "SoCKcMnNqPuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_clf.predict([some_digit])"
      ],
      "metadata": {
        "id": "HSz-OJyNo1wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
        "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
      ],
      "metadata": {
        "id": "IiqYH1qMqH5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – shows that we get a negligible performance improvement when we\n",
        "#              set average=\"weighted\" because the classes are already pretty\n",
        "#              well balanced.\n",
        "f1_score(y_multilabel, y_train_knn_pred, average=\"weighted\")"
      ],
      "metadata": {
        "id": "hKnCskOLulV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import ClassifierChain\n",
        "\n",
        "chain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\n",
        "chain_clf.fit(X_train[:2000], y_multilabel[:2000])\n",
        "chain_clf.predict([some_digit])"
      ],
      "metadata": {
        "id": "QC6wOL_Lu8AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier Chains are a simple yet powerful extension of the One-vs-Rest approach that lets you model label correlations in multi-label problems."
      ],
      "metadata": {
        "id": "QLA_JOJYtX48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MULTIOUTPUT CLASSIFICATION"
      ],
      "metadata": {
        "id": "zJTB3rrivx8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)  # to make this code example reproducible\n",
        "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
        "X_train_mod = X_train + noise\n",
        "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
        "X_test_mod = X_test + noise\n",
        "y_train_mod = X_train\n",
        "y_test_mod = X_test"
      ],
      "metadata": {
        "id": "XAXtvkMpvxHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – this cell generates and saves Figure 3–12\n",
        "plt.subplot(121); plot_digit(X_test_mod[0])\n",
        "plt.subplot(122); plot_digit(y_test_mod[0])\n",
        "save_fig(\"noisy_digit_example_plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hWHhIFQ-4RT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_train_mod, y_train_mod)\n",
        "clean_digit = knn_clf.predict([X_test_mod[0]])\n",
        "plot_digit(clean_digit)\n",
        "save_fig(\"cleaned_digit_example_plot\")  # extra code – saves Figure 3–13\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ehw1xIzf4MYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this approach isn't very good a multioutput regressor would be more natural here"
      ],
      "metadata": {
        "id": "ZiRPIhsA46Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5, weights=\"distance\")\n",
        "knn_reg.fit(X_train_mod, y_train_mod)           # y_train_mod = clean images\n",
        "clean_digit = knn_reg.predict([X_test_mod[0]])  # shape (1,784)\n",
        "plot_digit(clean_digit[0])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0raUUSTY5Egd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "example of multi label"
      ],
      "metadata": {
        "id": "oaEkr8q55qPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import hamming_loss, classification_report\n",
        "\n",
        "# 1️⃣ Generate a random multi-label dataset\n",
        "#    - n_samples=200 examples\n",
        "#    - n_features=20 input features\n",
        "#    - n_classes=3 possible labels per example\n",
        "#    - each example has on average 2 labels\n",
        "X, Y = make_multilabel_classification(\n",
        "    n_samples=200,\n",
        "    n_features=20,\n",
        "    n_classes=3,\n",
        "    n_labels=2,\n",
        "    allow_unlabeled=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2️⃣ Split into train / test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3️⃣ Wrap a base classifier for multi-output\n",
        "#    Here: one binary LogisticRegression per label\n",
        "base_clf = LogisticRegression(solver=\"liblinear\")\n",
        "multi_clf = OneVsRestClassifier(base_clf)\n",
        "\n",
        "# 4️⃣ Train\n",
        "multi_clf.fit(X_train, Y_train)\n",
        "\n",
        "# 5️⃣ Predict on test set\n",
        "Y_pred = multi_clf.predict(X_test)\n",
        "\n",
        "# 6️⃣ Evaluate\n",
        "print(\"Hamming loss:\", hamming_loss(Y_test, Y_pred))\n",
        "print(\"\\nClassification report (per label):\")\n",
        "print(classification_report(Y_test, Y_pred, target_names=[\n",
        "    \"Label A\", \"Label B\", \"Label C\"\n",
        "]))\n",
        "Y_pred[0]"
      ],
      "metadata": {
        "id": "OpJhC_Tp5n3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "categorical data + continous data"
      ],
      "metadata": {
        "id": "UtTl6jjA_WTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "\n",
        "# Suppose X_train is your features,\n",
        "# Y_cont (n_samples×n_cont) continuous targets,\n",
        "# Y_cat  (n_samples×n_cat) categorical targets.\n",
        "\n",
        "# 1) Regression head\n",
        "reg = MultiOutputRegressor(RandomForestRegressor())\n",
        "reg.fit(X_train, Y_cont)\n",
        "Y_cont_pred = reg.predict(X_test)\n",
        "\n",
        "# 2) Classification head\n",
        "clf = MultiOutputClassifier(RandomForestClassifier())\n",
        "clf.fit(X_train, Y_cat)\n",
        "Y_cat_pred = clf.predict(X_test)\n",
        "\n",
        "# 3) Combine\n",
        "import numpy as np\n",
        "Y_pred = np.hstack([Y_cont_pred, Y_cat_pred])\n"
      ],
      "metadata": {
        "id": "8TQcLvCT_II_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercice tunnig of hyperparametres"
      ],
      "metadata": {
        "id": "9luoktMe8nm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "0fOcYPjg7dcT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\",   KNeighborsClassifier())\n",
        "])\n",
        "param_grid = {\n",
        "    \"knn__n_neighbors\": [3, 5, 7, 9],\n",
        "    \"knn__weights\":     [\"uniform\", \"distance\"],\n",
        "    \"knn__p\":           [1, 2]            # 1=Manhattan, 2=Euclidean\n",
        "}\n",
        "grid = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "MXvggxkL8myz",
        "outputId": "2592022b-cfba-4ce0-d77a-ff7bb5ca6d23"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-2027620071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LxxBIY8RBL4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "évaluation des meilleur paramétres à l'aide de la cv et \\de la méthode de gridsearch + l'usage du scaler avec le knn car le knn decide de la closeness avec des metrics de distance but be aware it recommends high computional power"
      ],
      "metadata": {
        "id": "t-MezPSk9Ng1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐ whenever you consider using distance metrics think of scaling"
      ],
      "metadata": {
        "id": "6MEzeW1G-NY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"CV best accuracy: {:.3%}\".format(grid.best_score_))\n",
        "\n",
        "# 6️⃣ Evaluate on the held-out test set\n",
        "y_pred = grid.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: {:.3%}\".format(test_acc))"
      ],
      "metadata": {
        "id": "PVuCJix99Mon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN orther to improve accuracy we can do a data augmentation by shifting the images"
      ],
      "metadata": {
        "id": "AZF79u2HE_SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import shift\n",
        "def shift_image(image, dx, dy):\n",
        "    image = image.reshape((28, 28))\n",
        "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
        "    return shifted_image.reshape([-1])\n",
        "\n",
        "X_train_augmented = [image for image in X_train]\n",
        "y_train_augmented = [label for label in y_train]\n",
        "\n",
        "for dx, dy in ((-1, 0), (1, 0), (0, 1), (0, -1)):\n",
        "    for image, label in zip(X_train, y_train):\n",
        "        X_train_augmented.append(shift_image(image, dx, dy))\n",
        "        y_train_augmented.append(label)\n",
        "\n",
        "X_train_augmented = np.array(X_train_augmented)\n",
        "y_train_augmented = np.array(y_train_augmented)"
      ],
      "metadata": {
        "id": "NcSKDD1FGQti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code exemple for data augmentation"
      ],
      "metadata": {
        "id": "OBoQzmt-GhBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
        "X_train_augmented = X_train_augmented[shuffle_idx]\n",
        "y_train_augmented = y_train_augmented[shuffle_idx]"
      ],
      "metadata": {
        "id": "SGb-0qjVGTxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "very importanted this code ungroup the shifted images"
      ],
      "metadata": {
        "id": "FMC3iccTGoZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\",   KNeighborsClassifier(n_neighbors=5, weights=\"distance\", p=1))\n",
        "])\n",
        "pipe.fit(X_train_aug, y_train_aug)\n",
        "\n",
        "# 4️⃣ Evaluate on the original test set\n",
        "y_pred = pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy after data augmentation: {test_acc:.3%}\")"
      ],
      "metadata": {
        "id": "VUED8poLGuZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data augmentation is a very interesting way to improve the accuracy"
      ],
      "metadata": {
        "id": "dJGSz_m2Gy_P"
      }
    }
  ]
}
